---
title: "MovieLens Report"
author: "Aditya Chate"
date: "05/01/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


```{r rmd_data, include=FALSE}
#Loading required libraries for the script:
library(tidyverse)
library(gridExtra)
load("rmd_dat.RData")
```

## Overview:

The objective of the project comprised of creating a movie rating prediction 
system with the provided dataset. The "edx" and "validation" datasets were
created with code provided in the project introduction on the course website.
The "edx" dataset was used for data-exploration, analysis, and machine learning 
algorithm training purposes. With due consideration for computing capacity and 
computing time, a random sampling approach was employed. The predictions
generated were tested against the "validation" set to obtain an __RMSE__ (root mean
square error) of __0.69__.

The following linear model was fit to the data:

 __*y = mu + b_i + b_u + b_y + b_g + epsilon*__

where,   
      __*y*__ = __predicted rating,__  
      __*mu*__ = __average rating,__          
     __*b_i*__ = __movie bias,__    
     __*b_u*__ = __user bias,__  
     __*b_y*__ = __movie release year bias,__  
     __*b_g*__ = __genre bias,__    
 __*epsilon*__ = __independent errors from sampling from the same dataset.__  
 
The above variables required for the linear model represent sources of variation 
in rating prediction, and hence are considered as "biases". They were 
either directly obtained from the data on variables present in the dataset, or 
were derived by wrangling and processing the available data. A regularization 
parameter, *lambda* = 3.25, was obtained using a representative sample of 10000 
observations. This was used to regularize the predictions generated by an 
averaging approach. Readings for the "biases" and the predictions 
obtained by applying them one after the other, their regularized equivalents,
and the average ratings for the movies were used together as a set of 17 
predictors to train the machine learning models. Once again, with due 
consideration for computing time and computing capacity, six different sets of 
1000 samples each were used. Each set was trained on nine different individual
machine learning models, and one ensemble model. Rating prediction results of
each of those were checked against actual ratings for the same movies in the 
validation dataset. The RMSEs showed a __standard deviation__ of __0.022__, and all
results (individual RMSEs) were within two standard deviations of the overall 
mean RMSE, thus indicating good reproducibility of results. The __minimum RMSE__
obtained among all runs of the machine learning models was __0.65__, and the mean 
of all RMSEs, yielded a __final RMSE__ of __0.69__.  

\newpage

## Methodology and Analysis:   
  
### Definition of the RMSE(root mean square error) function:      
  
The RMSE(root mean square error) function calculates the difference between the
predicted value and the true (observed) value of a variable in a dataset which,
in this case, is the rating of a movie by a specific user. The function involves 
taking a square of the difference to obtain a positive number and then taking 
the square root, thus giving the absolute value of the difference. The average 
of these values is taken across the dataset to give the RMSE of the dataset. If
the difference between the predicted values and the actual observed values is
minimal, then the RMSE function will output a minimum value, thus indicating
the accuracy of the predictions. An RMSE of less than 0.86490 was expected
for full points in the final result section of the project.    

__RMSE <- function(true_ratings, predicted_ratings)__    
__{__  
   __sqrt(mean((true_ratings - predicted_ratings)^2))__  
__}__    

### Dataset Structures:  

The "edx" and the "validation" datasets are both created from the MovieLens
dataset using code provided on the course website. These are the datasets to be
used for analysis and algorithm training, and for validating the generated
predictions respectively. Examining the structure of the given datasets reveals each to be
comprising of observations on six variables, namely, *userId*, *movieId*, *rating*,
*timestamp*, *title*, and *genres*.  

__Structure of edx dataset for analysis and training of algorithms to generate predictions:__  

```{r str_edx, echo=FALSE}
str(edx_head)
```
__Structure of validation dataset for final RMSE evaluation:__    
```{r str_validation, echo=FALSE}
str(validation_head)
```
\newpage

__First six rows of the edx dataset for analysis and training of algorithms to generate predictions:__      

```{r head_edx, echo=FALSE}
edx_head
```
__First six rows of the validation dataset for final RMSE evaluation:__    
```{r head_validation, echo=FALSE}
validation_head
```
*Rating* being the variable of interest for which the predictions are to be
generated, the other data present in the dataset was examined in terms of its
distribution for having potential contribution in the variation in rating.  
\newpage
Distribution of the count of ratings received by each movie, and the count of
ratings provided by each user were examined directly by grouping the data
based on the *userId* and *movieId* variable. The *timestamp* variable was
converted to *rating_year* by means of the *lubridate* package. Additionally,
every entry in the *title* column has the movie title along with the year of it's
release written in brackets after it. This regular pattern of entries was put
to use for string processing using *regex* (regular expressions). Finally, to
quantify the effect based on genres, the following method was used:  

1. Each movie was observed to have either "(no genres listed)", or one or more 
genre entered in the *genres* column. Movies having more than one genre have 
all of them entered in the genre column separated using the vertical bar sign, 
"|", as a delimiter. 
2. A character vector was made by grouping entries in the *genres* column.
3. The *unnest_tokens* command was used to separate individual genre names,
applying necessary corrections wherever required, such as in cases of hyphenated
genre names. Thus, individual genres occuring across the entire dataframe were put
as single entries in one character vector.
4. An arbitrary score was assigned to each individual category based on the
order of occurrence in the created character vector, thus yielding the following
dataframe:
```{r genre_score_chart, echo=FALSE}
genre_score_chart
```
5. This score, while being nominal, served as a quantitative representation of
the occurrence of a specific genre in the entries of the *genres* variable.
6. Adding up individual scores for a particular entry in the *genres* column
gave a total genre score which was a quantitative representation of all the
genres that a particular movie belonged to. This was calculated using the
following function:  

__Function to define genre scores:__  
genre_score_fun <- function(g)  
{g_dat <- g %>% str_split(pattern = "\\\|") %>% .[[1]] %>% data.frame(genre = .)  
  genre_score <- sapply(g_dat, function(genre){  
    ind <- match(genre, genre_ score_ chart$ genres)  
    genre_score_chart$score[ind]})  
  sum(genre_score)}  
\newpage

## Distributions of selected variables:

The variables available in the original dataset, and those obtained by the
wrangling and processing of data as described were examined for their distribution
in the dataset to check if they have any kind of a skewed distribution, thus,
making them potential sources of variability in rating prediction.  

The variables considered were:  
* *movieId*  
* *userId*  
* *rating_year*  
* *movie_year* (movie release year)  
* *genre_score*  

## Variability in rating observed based on selected potential sources of variation:  


```{r variability_graphs, include=FALSE, echo=FALSE}
M
U
G
MY
RY
```
```{r variability_graphs_combined, echo=FALSE}
variability_graphs <- grid.arrange(M, U, MY, RY, G, ncol = 2)
```

The graphs obtained show that the variables *movieId*, *userId*  and *movie_year* 
are significantly skewed in their distribution, thus indicating a strong effect on 
the number of ratings. The *genre_score* variable shows some variability in it's
effect on the number of ratings, while the *rating_year* variable does not show
any significant variability.  

Hence, *movieId*, *userId*, *movie_year*, and *genre_score* were the variables
chosen for further analysis.
\newpage

The linear model, __*y = mu + b_i + b_u + b_y + b_g + epsilon*__, was applied 
using an averaging based approach. This was done as follows:  

* The average of all ratings in the dataset was calculated mu = 3.51   

* The movie bias is then calculated as: b_i = mean(rating - mu)

* The predictions using movie bias are then calculated as: pred_b_i = mu + b_i  

* This is repeated for the remaining variables as follows:

    + b_u = mean(rating - mu - b_i)

    + pred_b_u = mu + b_i + b_u

    + b_y = mean(rating - mu - b_i - b_u)

    + pred_b_y = mu + b_i + b_u + b_y

    + b_g = mean(rating - mu - b_i - b_u - b_y)

    + pred_b_g = mu + b_i + b_u + b_y + b_g  

Thus, we now construct a dataset which has the variables under consideration for
being sources of variation in rating prediction, the effects of the biases induced
by each of them, and the predictions generated by applying each individual bias 
to the average rating.

## Dataset with variables chosen as sources of variation in rating prediction:

```{r prediction_dataset, echo=FALSE}
str(edx_pred_dat_head)
edx_pred_dat_head
```
\newpage

## Lambda Optimization:  

The effects of the biases and the predictions generated by them as shown above
can be subject to regularization for better accuracy of prediction using a 
regularization parameter, *lambda*. For calculating the optimal value of *lambda*,
the following procedure was used:  

* A representative sample of 10000 ratings was chosen such that there were 1000
movies for each unique rating.  

* This was divided into a training and test set.

* A range of *lambdas* from 0 to 10 with an interval of 0.25 was created as a
numeric vector for each value to be tested individually for the RMSE value that
it would yield.

* For n() being the number of ratings and the letter "l" being the value of *lambda*, the
subsequent steps were followed.  

* The average of all ratings in the dataset was calculated mu = 3.51   

* The regularized movie bias is then calculated as: b_i_l = sum(rating - mu)/(n() + l)

* The regularized predictions using movie bias are then calculated as: pred_b_i_l = mu + b_i_l  

* This is repeated for the remaining variables as follows:

    + b_u_l = sum(rating - mu - b_i_l)/(n() + l)

    + pred_b_u_l = mu + b_i_l + b_u_l

    + b_y_l = sum(rating - mu - b_i_l - b_u_l)/(n() + l)

    + pred_b_y_l = mu + b_i_l + b_u_l + b_y_l

    + b_g_l = sum(rating - mu - b_i_l - b_u_l - b_y_l)/(n() + l)

    + pred_b_g_l = mu + b_i_l + b_u_l + b_y_l + b_g_l  

The following graph was obtained showing the RMSE for different values of *lambda*
across the selected range:

```{r lambda_optimization, echo=FALSE, fig.height=3.5, fig.width=6}
#Lambda Optimization Graph:
plot(lambdas, rmses, type = "b", xlab = "Lambda Value") 
title(main = "Lambda Optimization Curve")
points(lambdas, rmses, cex = .5, col = "dark red")
points(lambda , min(rmses), cex = 3, col = "red")
lines(lambdas, rmses, col = "blue")
text(x = lambda, y = min(rmses) + 0.04, labels = "3.25") 
text(x = lambda, y = min(rmses) + 0.08, labels = "Optimal Lambda Value")
```

It is seen that the RMSE was minimum at *lambda* = 3.25. This was the value used
to generate regularized effect bias and prediction data.
\newpage

## Regularized Prediction Data:

Using all the above results, the following dataset was generated:  

```{r edx_regularized, echo=FALSE}
str(edx_pred_dat_l_head)
edx_pred_dat_l_head
```

\newpage

## Machine Learning Algorithm Training:  

In the above dataset, the values for the variables *b_i*, *b_u*, *b_y*, *b_g*, 
*pred_b_i*, *pred_b_u*, *pred_b_y*, *pred_b_g*, *b_i_l*, *b_u_l*, *b_y_l*, 
*b_g_l*, *pred_b_i_l*, *pred_b_u_l*, *pred_b_y_l*, and *pred_b_g_l*, are all 
obtained by applying a set of uniform operations on the data previously present 
in the dataset. These variables would hence serve as a good predictor space for
rating predictions, and thus were used as such.

### Sample set creation:

Running machine learning algorithms on the full dataset being beyond the limitations
of computing time and computing capacity, a random sampling approach was used.
Random samples of 1000 movies were chosen for one run of each set of machine learning
algorithms. Six such samples were taken by setting the seed at different points
to obtain six different sample sets of 1000 movies each from the entire dataset. These 
were used to demonstrate consistency in reproducibility of results.  

One particular sample set thus has a total of 1000 movies, with the average
rating for each movie and the variables mentioned above as predictor space used
for training the machine learning algorithms.  

This sample set was then divided into a training set and test set comprising of
500 observations each.  

The principal component analysis (for the first sample) showed the following results:  

## Principal Component Analysis of machine learning training data:  

```{r pca, echo=FALSE}
summary(pca_1)
plot(pca_1$sdev, type = "b", xlab = "Principal Component Number")
title(main = "Principal Component Analysis")
points(pca_1$sdev, cex = .5, col = "dark red")
points(6 , pca_1$sdev[6], cex = 3, col = "red")
lines(pca_1$sdev, col = "blue")
axis(1, 0:17, col.axis = "blue")
text(x = 6, y = 0.2, labels = "PC6") 
text(x = 7.5, y = 0.3, labels = "Full Cumulative Proportion Of Variance")
```

The data and the graph show that the first six principal components account for 
all the variability of the data. Thus, PC1 to PC6 were selected to train the 
machine learning algorithms.  

Nine different linear classification models were chosen for machine learning training.
These were "bayesglm", "gaussprLinear", "glm", "glmStepAIC", "glmnet", "pls", "simpls", 
"kernelpls", and "widekernelpls". Additionally one ensemble model,  "randomGLM", was chosen.
These models each had *ncomp* (number of components) as the only tuning parameter. With this
parameter already being chosen optimally by principal component analysis, there was no further need for
optimization, thus reducing computing effort, and making the choice of these models aptly suited
for training and prediction purposes here.

\newpage

## Results:

The first run all all models yielded an average RMSE of 0.67 and a minimum RMSE of 0.66
as can be seen from following graph:  

### RMSE Graph:  
```{r result_graph, echo=FALSE}

#Graph showing RMSEs obtained by each model:

final_rmses_1 %>% ggplot(aes(Models, ML_RMSE, fill = Models)) + geom_col() +
  geom_hline(yintercept = 0.67, lty = 2, size = 1) +
  geom_text(aes(x = 0, y = 0.67, label = "Average RMSE = 0.67"), nudge_x = 2.1,
            nudge_y = -0.02) +
  geom_text(aes(label = rmse_rounded), nudge_y = 0.05) +
  geom_text(aes(label = signif(ML_RMSE, digits = 6)), 
            angle = 90, nudge_y = -0.4) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r result, echo=TRUE}
#Minimum RMSE obtained:
min(final_rmses_1$ML_RMSE)

#Model yielding the minimal RMSE:
min_ind <- which.min(final_rmses_1$ML_RMSE)
final_rmses_1[min_ind,]

#Average RMSE from all models:
result <- mean(final_rmses_1$ML_RMSE)
result
```

\newpage

### Reproduced results:  

To demonstrate consistency in reproducibility of results obtained by this method
all models were run on six different samples. All samples consistently showed
the first six components to account for all variability in the data as can be seen
from the PCA data and graphs below:  

### PCA_1: 
```{r pca_1, echo=FALSE}
summary(pca_1)
plot(pca_1$sdev, type = "b", xlab = "Principal Component Number")
title(main = "Principal Component Analysis #1")
points(pca_1$sdev, cex = .5, col = "dark red")
points(6 , pca_1$sdev[6], cex = 3, col = "red")
lines(pca_1$sdev, col = "blue")
axis(1, 0:17, col.axis = "blue")
text(x = 6, y = 0.2, labels = "PC6") 
text(x = 7.5, y = 0.3, labels = "Full Cumulative Proportion Of Variance")
```

\newpage

### PCA_2:  
```{r pca_2, echo=FALSE}
summary(pca_2)
plot(pca_2$sdev, type = "b", xlab = "Principal Component Number")
title(main = "Principal Component Analysis #2")
points(pca_2$sdev, cex = .5, col = "dark red")
points(6 , pca_2$sdev[6], cex = 3, col = "red")
lines(pca_2$sdev, col = "blue")
axis(1, 0:17, col.axis = "blue")
text(x = 6, y = 0.2, labels = "PC6") 
text(x = 7.5, y = 0.3, labels = "Full Cumulative Proportion Of Variance")
```

\newpage


### PCA_3:  
```{r pca_3, echo=FALSE}
summary(pca_3)
plot(pca_3$sdev, type = "b", xlab = "Principal Component Number")
title(main = "Principal Component Analysis #3")
points(pca_3$sdev, cex = .5, col = "dark red")
points(6 , pca_3$sdev[6], cex = 3, col = "red")
lines(pca_3$sdev, col = "blue")
axis(1, 0:17, col.axis = "blue")
text(x = 6, y = 0.2, labels = "PC6") 
text(x = 7.5, y = 0.3, labels = "Full Cumulative Proportion Of Variance")
```

\newpage


### PCA_4:  
```{r pca_4, echo=FALSE}
summary(pca_4)
plot(pca_4$sdev, type = "b", xlab = "Principal Component Number")
title(main = "Principal Component Analysis #4")
points(pca_4$sdev, cex = .5, col = "dark red")
points(6 , pca_4$sdev[6], cex = 3, col = "red")
lines(pca_4$sdev, col = "blue")
axis(1, 0:17, col.axis = "blue")
text(x = 6, y = 0.2, labels = "PC6") 
text(x = 7.5, y = 0.3, labels = "Full Cumulative Proportion Of Variance")
```

\newpage


### PCA_5:  
```{r pca_5, echo=FALSE}
summary(pca_5)
plot(pca_5$sdev, type = "b", xlab = "Principal Component Number")
title(main = "Principal Component Analysis #5")
points(pca_5$sdev, cex = .5, col = "dark red")
points(6 , pca_5$sdev[6], cex = 3, col = "red")
lines(pca_5$sdev, col = "blue")
axis(1, 0:17, col.axis = "blue")
text(x = 6, y = 0.2, labels = "PC6") 
text(x = 7.5, y = 0.3, labels = "Full Cumulative Proportion Of Variance")
```

\newpage


### PCA_6:  
```{r pca_6, echo=FALSE}
summary(pca_6)
plot(pca_6$sdev, type = "b", xlab = "Principal Component Number")
title(main = "Principal Component Analysis #6")
points(pca_6$sdev, cex = .5, col = "dark red")
points(6 , pca_6$sdev[6], cex = 3, col = "red")
lines(pca_6$sdev, col = "blue")
axis(1, 0:17, col.axis = "blue")
text(x = 6, y = 0.2, labels = "PC6") 
text(x = 7.5, y = 0.3, labels = "Full Cumulative Proportion Of Variance")
```

\newpage

### Final Results:  

All runs of all models on each sample set show RMSEs being within two standard
deviations of the average RMSE, 0.69, and a standard deviation as low as 0.022,
thus indicating good accuracy of models and consistent reproducibility of 
results, as can be seen from the graph below:  

### Graph showing results of all sample analyses combined:  

```{r final_graph, echo=FALSE}
sd_rmse <- sd(f_rmse_chart$ML_RMSE)
f_rmse_chart %>% ggplot(aes(Models, ML_RMSE, fill = Models)) + geom_col() +
  geom_errorbar(aes(ymin = 0.69 - 2*sd_rmse, ymax = 0.69 + 2*sd_rmse)) +
  geom_hline(yintercept = 0.69, lty = 2, size = 1) +
  geom_text(aes(x = 0, y = 0.69, label = "Final RMSE = 0.69"), nudge_x = 6,
            nudge_y = -0.1) +
  geom_text(aes(label = rmse_rounded), angle = 90, nudge_y = -0.4) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  facet_wrap(~sample_num)
```



```{r final_results, echo=TRUE}
# Minimum RMSE obtained:
min(f_rmse_chart$ML_RMSE)

# Model yielding the minimal RMSE:
min_ind <- which.min(f_rmse_chart$ML_RMSE)
f_rmse_chart[min_ind,]

# Standard Deviation of RMSEs from all models on all samples:
sd_rmse

# Average RMSE from all models:
final_result <- mean(f_rmse_chart$ML_RMSE)
final_result
```

## Conclusion:  

Among the variables present in the provided dataset, the *movieId* and *userId*
variables were clear sources of variability in ratings of movies. The genre(s)
and release year of the movies contributed to additional variability. With these
variables in consideration, the linear model as described previously was fit to
the data:  

__*y = mu + b_i + b_u + b_y + b_g + epsilon*__

where,   
      __*y*__ = __predicted rating,__  
      __*mu*__ = __average rating,__          
     __*b_i*__ = __movie bias,__    
     __*b_u*__ = __user bias,__  
     __*b_y*__ = __movie release year bias,__  
     __*b_g*__ = __genre bias,__    
 __*epsilon*__ = __independent errors from sampling from the same dataset.__  
 
 The various "biases" and the predictions obtained by applying them across the
 dataset, along with their equivalents regularized with the optimized regularization parameter,
 *lambda* = 3.25, served as a proper predictor space for rating prediction.
 Principal component analysis of this predictor space showed the first six
 principal components to account for all variability in the data, hence these
 six principal components were chosen to train machine learning algorithms on.  
 
 The results were reproduced six times with different samples, yielding an 
 average __final RMSE__ of __0.69__, and showing a __standard deviation__ as low as __0.022__, thus 
 indicating good reproducibility and accuracy of this method.  
 
 The limitations of this analysis mainly comprised of limited sample sizes (10000
 for *lambda* optimization and 1000 for each set of machine learning models).
 While the method's consistent reproducibility was demonstrated by repeated
 random sampling and a low standard deviation (0.022) in the results obtained,
 there is no guaranteed indicator of scalability for larger datasets. Limited
 computing capacity imposed these restrictions on sample sizes and computing time.
 Future goals would involve testing this method on larger datasets, but this can
 only be done with an increase in computing capacity beyond what is available with
 regular commercial laptops, and would require more powerful workstations.
 

